# import streamlit as st
# import os
# from openai import AzureOpenAI
# from dotenv import load_dotenv
# # ì‹œê°„ ì¶”ê°€
# from datetime import datetime
# import pytz

# # 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì´ ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•¨)
# load_dotenv()

# search_index = ("SEARCH_INDEX_NAME", "azure-rag")
# search_key = os.getenv("SEARCH_KEY")
# search_endpoint = os.getenv("SEARCH_ENDPOINT")
# semantic_config = os.getenv("SEMANTIC_CONFIG", "azure-rag-semantic-configuration")
# st.title("Azure Expert")

# # 2. Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# # (ì‹¤ì œ ê°’ì€ .env íŒŒì¼ì´ë‚˜ ì—¬ê¸°ì— ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”)
# client = AzureOpenAI(
#     api_key=os.getenv("AZURE_OAI_KEY"),
#     api_version="2024-05-01-preview",
#     azure_endpoint=os.getenv("AZURE_OAI_ENDPOINT"),

# )

# # 3. ëŒ€í™”ê¸°ë¡(Session State) ì´ˆê¸°í™” - ì´ê²Œ ì—†ìœ¼ë©´ ìƒˆë¡œê³ ì¹¨ ë•Œë§ˆë‹¤ ëŒ€í™”ê°€ ë‚ ì•„ê°‘ë‹ˆë‹¤!
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# # 4. í™”ë©´ì— ê¸°ì¡´ ëŒ€í™” ë‚´ìš© ì¶œë ¥
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.markdown(message["content"])

# # 5. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
# if prompt := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
#     # (1) ì‚¬ìš©ì ë©”ì‹œì§€ í™”ë©´ì— í‘œì‹œ & ì €ì¥
#     st.chat_message("user").markdown(prompt)
#     st.session_state.messages.append({"role": "user", "content": prompt})

#     # ì‹œê°„ ì²´í¬ ë¡œì§ ì¶”ê°€
#     processed_prompt = prompt.lower()
#     import re
#     processed_prompt = re.sub(r'[\s\.\?!]', '', processed_prompt)  # ê³µë°± ì œê±°
#     time_keywords_clean = ['ì‹œê°„', 'ëª‡ì‹œ', 'í˜„ì¬ì‹œê°', 'ì§€ê¸ˆëª‡ì‹œ', 'ì˜¤ëŠ˜ë‚ ì§œ', 'ì˜¤ëŠ˜ì´ë©°ì¹ ', 'ì˜¤ëŠ˜ì€ëª‡ì›”ë©°ì¹ ', 'ì˜¤ëŠ˜ìš”ì¼']

#     is_time_request = any(keyword in processed_prompt for keyword in time_keywords_clean)
#     assistant_reply = ""
#     if is_time_request:
#         tz = pytz.timezone('Asia/Seoul')
#         # %Yë…„ %mì›” %dì¼
#         current_time = datetime.now(tz).strftime('%Hì‹œ %Më¶„')
#         assistant_reply = f"í˜„ì¬ ì‹œê°„ì€ {current_time}ì…ë‹ˆë‹¤."
#     else:
#         # (2) AI ì‘ë‹µ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì•„ë‹˜, ë‹¨ìˆœ í˜¸ì¶œ ì˜ˆì‹œ)
#         messages_payload=[
#             {"role": m["role"], "content": m["content"]}
#             for m in st.session_state.messages
#         ]
#         with st.spinner("ìƒê°ì¤‘..."):
#             try:

#                 response = client.chat.completions.create(
#                     model="gpt-4o-mini",
#                     messages=messages_payload,
#                     max_tokens=6553,
#                     temperature=0.7,
#                     top_p=0.95,
#                     frequency_penalty=0,
#                     presence_penalty=0,
#                     stop=None,
#                     stream=False,
#                     extra_body={
#                     "data_sources": [{
#                         "type": "azure_search",
#                         "parameters": {
#                             "endpoint": f"{search_endpoint}",
#                             "index_name": "azure-rag",
#                             "semantic_configuration": "azure-rag-semantic-configuration",
#                             "query_type": "vector_semantic_hybrid",
#                             "fields_mapping": {},
#                             "in_scope": True,
#                             "filter": None,
#                             "strictness": 3,
#                             "top_n_documents": 5,
#                             "authentication": {
#                             "type": "api_key",
#                             "key": f"{search_key}"
#                             },
#                             "embedding_dependency": {
#                             "type": "deployment_name",
#                             "deployment_name": "text-embedding-ada-002"
#                             }
#                         }
#                         }]
#                     }
#                 )
#                 assistant_reply = response.choices[0].message.content
#             except Exception as e:
#                 assistant_reply = f"Azure RAG í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: ì„¤ì • ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: `{e}`"

#     # (3) AI ì‘ë‹µ ì €ì¥
#     with st.chat_message("assistant"):
#         st.markdown(assistant_reply)

#     st.session_state.messages.append({"role": "assistant", "content": assistant_reply})



import streamlit as st
import os
from openai import AzureOpenAI
from dotenv import load_dotenv
from datetime import datetime
import pytz
import re
import json # JSON íŒŒì‹±ì„ ìœ„í•´ ì¶”ê°€

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì´ ê°™ì€ í´ë”ì— ìˆì–´ì•¼ í•¨)
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
AZURE_OAI_KEY = os.getenv("AZURE_OAI_KEY")
AZURE_OAI_ENDPOINT = os.getenv("AZURE_OAI_ENDPOINT")
SEARCH_KEY = os.getenv("SEARCH_KEY")
SEARCH_ENDPOINT = os.getenv("SEARCH_ENDPOINT")
SEARCH_INDEX_NAME = os.getenv("SEARCH_INDEX_NAME", "azure-rag")
SEMANTIC_CONFIG = os.getenv("SEMANTIC_CONFIG", "azure-rag-semantic-configuration")
EMBEDDING_DEPLOYMENT_NAME = os.getenv("EMBEDDING_DEPLOYMENT_NAME", "text-embedding-ada-002")

# 2. Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ì•± ì „ì²´ì—ì„œ ê³µìœ )
try:
    client = AzureOpenAI(
        api_key=AZURE_OAI_KEY,
        api_version="2024-05-01-preview",
        azure_endpoint=AZURE_OAI_ENDPOINT,
    )
except Exception as e:
    st.error(f"Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”. ({e})")
    st.stop()


## UI ë° ì„¸ì…˜ ê´€ë¦¬ í•¨ìˆ˜
# ìƒˆ ëŒ€í™” ì‹œì‘ í•¨ìˆ˜
def new_chat():
    """ì„¸ì…˜ ìƒíƒœì˜ ë©”ì‹œì§€ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    st.session_state.messages = []
    st.session_state.error_message = None

# Streamlit UI ì„¤ì •
st.set_page_config(page_title="Azure Expert RAG Chatbot", layout="wide")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì • ë° ì œì–´")
    
    st.info(f"""
        **í˜„ì¬ í™˜ê²½:**
        - **ëª¨ë¸:** {os.getenv("AZURE_OAI_MODEL", "gpt-4o-mini")}
        - **Search Index:** `{SEARCH_INDEX_NAME}`
        """)
    
    # ìƒˆ ëŒ€í™” ë²„íŠ¼
    if st.button("ğŸ†• ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘", use_container_width=True):
        new_chat()

st.title("ğŸ“˜ Azure Expert RAG Chatbot")

# 3. ëŒ€í™”ê¸°ë¡(Session State) ì´ˆê¸°í™”
if "messages" not in st.session_state:
    new_chat() # ì´ˆê¸°í™” í•¨ìˆ˜ ì¬ì‚¬ìš©

# 4. í™”ë©´ì— ê¸°ì¡´ ëŒ€í™” ë‚´ìš© ì¶œë ¥
for message in st.session_state.messages:
    # ì‚¬ìš©ì ë©”ì‹œì§€
    if message["role"] == "user":
        with st.chat_message("user"):
            st.markdown(message["content"])
    # AI ë©”ì‹œì§€ (ì¶œì²˜ í¬í•¨)
    elif message["role"] == "assistant":
        with st.chat_message("assistant"):
            st.markdown(message["content"])
            if "sources" in message and message["sources"]:
                with st.expander("ğŸ”— **ì°¸ì¡° ë¬¸ì„œ ì¶œì²˜**"):
                    for source in message["sources"]:
                        st.markdown(source)

# 5. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸° ë° ì‘ë‹µ ìƒì„±
if prompt := st.chat_input("Azure í´ë¼ìš°ë“œ ê´€ë ¨ ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?"):
    # (1) ì‚¬ìš©ì ë©”ì‹œì§€ í™”ë©´ì— í‘œì‹œ & ì €ì¥
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # ì‹œê°„ ì²´í¬ ë¡œì§ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    processed_prompt = prompt.lower()
    processed_prompt = re.sub(r'[\s\.\?!]', '', processed_prompt) 
    time_keywords_clean = ['ì‹œê°„', 'ëª‡ì‹œ', 'í˜„ì¬ì‹œê°', 'ì§€ê¸ˆëª‡ì‹œ', 'ì˜¤ëŠ˜ë‚ ì§œ', 'ì˜¤ëŠ˜ì´ë©°ì¹ ', 'ì˜¤ëŠ˜ì€ëª‡ì›”ë©°ì¹ ', 'ì˜¤ëŠ˜ìš”ì¼']
    is_time_request = any(keyword in processed_prompt for keyword in time_keywords_clean)

    assistant_reply_content = ""
    sources_list = []

    if is_time_request:
        tz = pytz.timezone('Asia/Seoul')
        current_time = datetime.now(tz).strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')
        assistant_reply_content = f"í˜„ì¬ í•œêµ­ ì‹œê°„ì€ **{current_time}**ì…ë‹ˆë‹¤."
    else:
        # (2) AI ì‘ë‹µ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ë° RAG í¬í•¨)
        messages_payload=[
            {"role": m["role"], "content": m["content"]}
            for m in st.session_state.messages
        ]
        
        # Streamlit Chat Element ìƒì„± (ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´ placeholder ì—­í• )
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            try:
                # API í˜¸ì¶œ (stream=Trueë¡œ ë³€ê²½)
                response = client.chat.completions.create(
                    model=os.getenv("AZURE_OAI_MODEL", "gpt-4o-mini"),
                    messages=messages_payload,
                    max_tokens=6553,
                    temperature=0.7,
                    stream=True, # **ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”**
                    extra_body={
                        "data_sources": [{
                            "type": "azure_search",
                            "parameters": {
                                "endpoint": SEARCH_ENDPOINT,
                                "index_name": SEARCH_INDEX_NAME,
                                "semantic_configuration": SEMANTIC_CONFIG,
                                "query_type": "vector_semantic_hybrid",
                                "in_scope": True,
                                "top_n_documents": 5,
                                "authentication": {
                                    "type": "api_key",
                                    "key": SEARCH_KEY
                                },
                                "embedding_dependency": {
                                    "type": "deployment_name",
                                    "deployment_name": EMBEDDING_DEPLOYMENT_NAME
                                }
                            }
                        }]
                    }
                )
                
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                for chunk in response:
                    content = chunk.choices[0].delta.content
                    if content:
                        full_response += content
                        message_placeholder.markdown(full_response + "â–Œ") # ì»¤ì„œ íš¨ê³¼
                
                # ìµœì¢… ì‘ë‹µ ë‚´ìš© ì—…ë°ì´íŠ¸
                assistant_reply_content = full_response
                message_placeholder.markdown(assistant_reply_content)

                # RAG ì†ŒìŠ¤ ì¶”ì¶œ (Chat Completions APIì˜ ê²½ìš°, ì†ŒìŠ¤ ì •ë³´ê°€ 'context' í•„ë“œì— í¬í•¨ë˜ì–´ ë‚˜ì˜´)
                # ì´ ì •ë³´ëŠ” ìŠ¤íŠ¸ë¦¬ë°ì˜ ê²½ìš° ë§ˆì§€ë§‰ ì²­í¬ ë˜ëŠ” ë³„ë„ì˜ ë¡œì§ìœ¼ë¡œ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
                # í˜„ì¬ Azure OpenAI Python SDKëŠ” ìŠ¤íŠ¸ë¦¬ë° ì‹œ ì†ŒìŠ¤ ì¶”ì¶œì´ ë³µì¡í•˜ë¯€ë¡œ,
                # ì—¬ê¸°ì„œëŠ” ì‘ë‹µ ë¬¸ìì—´ì—ì„œ ì¸ìš©êµ¬(citations)ë¥¼ ì°¾ì•„ í‘œì‹œí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

                sources_pattern = r'\[doc\d{1,2}\]|\\[doc\d{1,2}\]' # ì˜ˆì‹œ: [doc1]
                citations = re.findall(sources_pattern, assistant_reply_content)
                
                # ì‹¤ì œ RAG ì†ŒìŠ¤ ì •ë³´ëŠ” API ì‘ë‹µì˜ 'context' í•„ë“œì— ë” ìì„¸íˆ ë‹´ê²¨ ìˆì§€ë§Œ,
                # Streamlit UI ìƒì˜ ê°„í¸ì„±ì„ ìœ„í•´ API ì‘ë‹µì˜ **íŠ¹ì • í•„ë“œ**ì—ì„œ ì§ì ‘ ì¶”ì¶œí•©ë‹ˆë‹¤.
                # ì‹¤ì œ êµ¬í˜„ ì‹œì—ëŠ” API ì‘ë‹µì—ì„œ 'context' (tool_calls) í•„ë“œë¥¼ íŒŒì‹±í•˜ëŠ” ì½”ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.
                # ì„ì‹œì ìœ¼ë¡œ, Streamlitì˜ Expanderì— "ì¶œì²˜ ì •ë³´ê°€ í¬í•¨ëœ ì‘ë‹µì„ ë³´ë ¤ë©´ ì „ì²´ API ì‘ë‹µì„ íŒŒì‹±í•´ì•¼ í•©ë‹ˆë‹¤."ë¼ëŠ” ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
                
                # (ì‹¤ì œ RAG ì¶œì²˜ í‘œì‹œ ë¡œì§: ì‹¤ì œ API ì‘ë‹µ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ context/tool_callsë¥¼ íŒŒì‹±í•´ì•¼ í•¨)
                # í˜„ì¬ Streamlit Chatbot êµ¬ì¡°ìƒ íŒŒì‹±ì´ ë³µì¡í•˜ì—¬ ì„ì‹œ ë©”ì‹œì§€ í‘œì‹œ
                
                # ì‹¤ì œ API ì‘ë‹µì—ì„œ contextë¥¼ íŒŒì‹±í–ˆë‹¤ê³  ê°€ì •í•˜ê³  ì„ì‹œ URL ëª©ë¡ì„ ì¶”ê°€
                # ì‹¤ì œ RAG ì†ŒìŠ¤ íŒŒì‹±ì´ ë³µì¡í•˜ì—¬, ì´ ì˜ˆì‹œì—ì„œëŠ” ì‘ë‹µì— í¬í•¨ëœ **ì¸ìš© ë²ˆí˜¸**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ì‹œ URL ëª©ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤.
                if citations:
                    st.info("ğŸ’¡ **ì°¸ê³ :** ì •í™•í•œ ì¶œì²˜ URLì„ í‘œì‹œí•˜ë ¤ë©´ API ì‘ë‹µì˜ JSONì„ íŒŒì‹±í•´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ëŠ” ì¸ìš© ë²ˆí˜¸ë§Œ í‘œì‹œë©ë‹ˆë‹¤.")
                    # ì„ì‹œ ì†ŒìŠ¤ ëª©ë¡ ìƒì„± (ì‹¤ì œ êµ¬í˜„ ì‹œ API ì‘ë‹µì—ì„œ ì¶”ì¶œ)
                    # ì´ ë¶€ë¶„ì„ ì™„ì„±í•˜ë ¤ë©´, response ê°ì²´ì˜ .tool_calls ë˜ëŠ” .context í•„ë“œë¥¼ íŒŒì‹±í•´ì•¼ í•©ë‹ˆë‹¤.
                    # í˜„ì¬ ì½”ë“œëŠ” íŒŒì‹± ë¡œì§ì´ ì—†ìœ¼ë¯€ë¡œ ì„ì‹œ ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
                    
                    sources_list.append("íŒŒì‹±ëœ ì‹¤ì œ ì†ŒìŠ¤ (ì˜ˆ: [Azure Document URL])")
                    sources_list.append("Azure AI Searchì—ì„œ ê²€ìƒ‰ëœ Top 5 ë¬¸ì„œ ì œëª©")
                    
                    with st.expander("ğŸ”— **ì°¸ì¡° ë¬¸ì„œ ì¶œì²˜ (íŒŒì‹± í•„ìš”)**"):
                        st.markdown("âœ… **ì°¸ì¡°ëœ ë¬¸ì„œ ë²ˆí˜¸:** " + ", ".join(sorted(list(set(citations)))))
                        st.markdown("> **ì°¸ê³ :** ì •í™•í•œ ì¶œì²˜ URLì„ ì–»ìœ¼ë ¤ë©´ `response.choices[0].message.context` (ë˜ëŠ” `tool_calls`) í•„ë“œë¥¼ íŒŒì‹±í•˜ëŠ” ì½”ë“œë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.")

            except Exception as e:
                assistant_reply_content = f"âš ï¸ **Azure RAG í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:** ì„¤ì • ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: `{e}`"
                message_placeholder.markdown(assistant_reply_content)
                st.session_state.error_message = assistant_reply_content

    # (3) AI ì‘ë‹µ ì €ì¥ (ì‹œê°„ ìš”ì²­ì´ë“ , AI ì‘ë‹µì´ë“ )
    # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì— ìµœì¢… ë‹µë³€ ì €ì¥ (ì†ŒìŠ¤ ì •ë³´ê°€ ìˆì„ ê²½ìš° í•¨ê»˜ ì €ì¥)
    message_to_save = {"role": "assistant", "content": assistant_reply_content}
    if sources_list:
        message_to_save["sources"] = sources_list
        
    st.session_state.messages.append(message_to_save)
